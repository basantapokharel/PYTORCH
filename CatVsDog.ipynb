{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18811,
     "status": "ok",
     "timestamp": 1739287889844,
     "user": {
      "displayName": "Sumit Lasiwa",
      "userId": "07890024866047393627"
     },
     "user_tz": -345
    },
    "id": "rvZWhyatMlpA",
    "outputId": "912c9528-f0f7-4197-bc1c-8eaf011e8248"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739287889848,
     "user": {
      "displayName": "Sumit Lasiwa",
      "userId": "07890024866047393627"
     },
     "user_tz": -345
    },
    "id": "BXIzbbW_IjK_"
   },
   "outputs": [],
   "source": [
    "# !unzip \"/content/drive/My Drive/Pet_images.zip\" -d \"/content/drive/My Drive/PetImages/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 12585,
     "status": "ok",
     "timestamp": 1739287902434,
     "user": {
      "displayName": "Sumit Lasiwa",
      "userId": "07890024866047393627"
     },
     "user_tz": -345
    },
    "id": "3Ccle8EgEwnx"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1739287902471,
     "user": {
      "displayName": "Sumit Lasiwa",
      "userId": "07890024866047393627"
     },
     "user_tz": -345
    },
    "id": "Gw-jxo0JFnye"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),   # 227 * 227 -->alexnet   # 224 * 224 --> vgg, resnet\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.datasets import ImageFolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 48587,
     "status": "ok",
     "timestamp": 1739287951056,
     "user": {
      "displayName": "Sumit Lasiwa",
      "userId": "07890024866047393627"
     },
     "user_tz": -345
    },
    "id": "XIsdoOv2JRFA"
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(r\"C:\\Users\\acer\\Downloads\\PetImages\\PetImages\\train\", transform)\n",
    "val_dataset = ImageFolder(r\"C:\\Users\\acer\\Downloads\\PetImages\\PetImages\\val\", transform)\n",
    "test_dataset = ImageFolder(r\"C:\\Users\\acer\\Downloads\\PetImages\\PetImages\\test\", transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739287951071,
     "user": {
      "displayName": "Sumit Lasiwa",
      "userId": "07890024866047393627"
     },
     "user_tz": -345
    },
    "id": "kR65h5vdHISE"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=64 ,\n",
    "                              shuffle = True,\n",
    "                              num_workers=2,\n",
    "                              pin_memory=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(train_dataset,\n",
    "                            batch_size=64 ,\n",
    "                            shuffle = False,\n",
    "                            num_workers=2,\n",
    "                            pin_memory=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(train_dataset,\n",
    "                             batch_size=64 ,\n",
    "                             shuffle = False,\n",
    "                             num_workers=2,\n",
    "                             pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_pzXOcmyGC_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for image, _ in train_dataloader:\n",
    "      print(image.shape)\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlOOMTqu9uB1"
   },
   "source": [
    "AlexNet Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "error",
     "timestamp": 1739287992060,
     "user": {
      "displayName": "Sumit Lasiwa",
      "userId": "07890024866047393627"
     },
     "user_tz": -345
    },
    "id": "nrZBJ0uW9odi",
    "outputId": "023d66bc-749b-4d75-b20b-dc2245a13e6c"
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.feature_extractor = nn.Sequential(\n",
    "        nn.Conv2d(3, 96, kernel_size=11, stride=4),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        nn.Conv2d(256, 384, kernel_size=3, stride=1, padding = 1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(384, 384, kernel_size=3, stride=1,padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(384, 256, kernel_size=3, stride=1,padding=1),   # (384, 13, 13) -> (256, 13, 13)\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(3, stride=2),      # (256, 13, 13) -> (256, 6, 6)\n",
    "        nn.Flatten()\n",
    "    )\n",
    "\n",
    "    self.fcn = nn.Sequential(\n",
    "        nn.Linear(9216,4096 ),      # 256 * 6 * 6 = 9216\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(4096, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "      feature_map = self.feature_extractor(x)\n",
    "      output = self.fcn(feature_map)\n",
    "      return output.shape\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image, _ in train_dataloader:\n",
    "#       image = image[0].unsqueeze(0)\n",
    "#       print({alexnet(image)})\n",
    "#       break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VGG16(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(VGG16, self). __init__()\n",
    "\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 1),   #<---224 *224 *3\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.MaxPool2d(kernel_size = 2, stride = 2) ,          #----> 112 * 112 * 64\n",
    "\n",
    "#             nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.Conv2d(128, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.MaxPool2d(kernel_size = 2, stride = 2) ,     #---> 56 * 56 * 128\n",
    "\n",
    "#             nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.Conv2d(256, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.Conv2d(256, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.MaxPool2d(kernel_size = 2, stride = 2),      #---> 28 * 28 * 256\n",
    "\n",
    "#             nn.Conv2d(256, 512, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.MaxPool2d(kernel_size = 2, stride = 2),      #---> 14 * 14 * 512\n",
    "\n",
    "\n",
    "#             nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.MaxPool2d(kernel_size = 2, stride = 2),      #--> 7 * 7 * 512\n",
    "\n",
    "#             nn.Flatten()     \n",
    "#         )\n",
    "\n",
    "#         self.fcn = nn.Sequential(\n",
    "#             nn.Linear(7 * 7 * 512, 4096),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.Linear(4096, 1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         feature_map = self.features(x)\n",
    "#         output = self.fcn(feature_map)\n",
    "#         return output.shape\n",
    "\n",
    "# vgg16 = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image, _ in train_dataloader:\n",
    "#     image = image[0].unsqueeze(0)\n",
    "#     print(vgg16(image))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(vgg16, input_size=(1, 3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "criteria = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, images, labels):\n",
    "    #forward pass\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    #make sure images shape is right\n",
    "    images = images.reshape(-1, 3, 224, 224)\n",
    "    outputs = model(images)\n",
    "    # print(\"shpae of outputs\",outputs.shape)\n",
    "    # print(\"shpae of labes is \",labels.shape)\n",
    "    #make sure the shape of outputs and labels are same\n",
    "    outputs = outputs.squeeze(1)\n",
    "    loss = criteria(outputs, labels)\n",
    "\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, outputs\n",
    "\n",
    "def validate_one_step(model, criteria, images, labels):\n",
    "    #forward pass\n",
    "    model.eval()\n",
    "    outputs = model(images)\n",
    "    loss = criteria(outputs, labels)\n",
    "\n",
    "    return loss,outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1 of 10:   0%|          | 0/274 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x6400 and 9216x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      8\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m     loss,results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     epoch_train_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     11\u001b[0m     results \u001b[38;5;241m=\u001b[39m sigmoid(results)\u001b[38;5;241m.\u001b[39mround()\n",
      "Cell \u001b[1;32mIn[34], line 7\u001b[0m, in \u001b[0;36mtrain_one_step\u001b[1;34m(model, optimizer, images, labels)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#make sure images shape is right\u001b[39;00m\n\u001b[0;32m      6\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(\"shpae of outputs\",outputs.shape)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# print(\"shpae of labes is \",labels.shape)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#make sure the shape of outputs and labels are same\u001b[39;00m\n\u001b[0;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\pytorch\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\pytorch\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[17], line 31\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     30\u001b[0m     feature_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(x)\n\u001b[1;32m---> 31\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\pytorch\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\pytorch\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\pytorch\\env\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\pytorch\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\pytorch\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\pytorch\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x6400 and 9216x4096)"
     ]
    }
   ],
   "source": [
    "train_loss, validation_loss = [], []\n",
    "train_accuracy, validation_accuracy = [], []\n",
    "for epoch in range(1,epochs +1):\n",
    "    # Training\n",
    "    epoch_train_loss = 0\n",
    "    epoch_correct_prediction = 0\n",
    "    for images, labels in tqdm(train_dataloader, desc=f'Training {epoch} of {epochs}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        loss,results = train_one_step(model,optimizer,images,labels)\n",
    "        epoch_train_loss+= loss.item()\n",
    "        results = sigmoid(results).round()\n",
    "        print(results)\n",
    "        break\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "model = AlexNet()\n",
    "criteria = nnBCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOC+mlJrxuYAUVphlXKQTnA",
   "gpuType": "T4",
   "mount_file_id": "1tYKWirXfscJN-QH66WS8l8Py3Flg30tE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
